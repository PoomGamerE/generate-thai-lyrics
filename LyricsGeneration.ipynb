{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generate Thai Lyrics**\n",
    "\n",
    "Most of the code is based on Udacity Deep Learning Github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3-us-west-2.amazonaws.com/thai-corpus/lyric_dataframe_update.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from pythainlp import word_tokenize\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "# !wget https://github.com/Phonbopit/sarabun-webfont/raw/master/fonts/thsarabunnew-webfont.ttf\n",
    "# !sudo cp thsarabunnew-webfont.ttf /usr/local/lib/python3.6/dist-packages/matplotlib/mpl-data/fonts/ttf/\n",
    "# !sudo cp thsarabunnew-webfont.ttf /usr/share/fonts/truetype/\n",
    "\n",
    "matplotlib.font_manager._rebuild()\n",
    "matplotlib.rc('font', family='TH Sarabun New')\n",
    "matplotlib.rc('figure', figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(ls):\n",
    "    \"\"\"Flatten list of list\"\"\"\n",
    "    return list(chain.from_iterable(ls))\n",
    "\n",
    "def clean_lyrics(lyric):\n",
    "    \"\"\"Clean lines that do not contain lyrics\"\"\"\n",
    "    lines = lyric.split('\\n')\n",
    "    lyrics_clean = [] \n",
    "    for line in lines:\n",
    "        # remove headers from the file\n",
    "        headers = [\n",
    "            'เพลง ', 'คำร้อง ', 'คำร้อง/ทำนอง ', 'ศิลปิน ', 'ทำนอง ', \n",
    "            'เรียบเรียง ', 'เพลงประกอบละคร ', 'อัลบัม ', 'ร่วมร้องโดย ', \n",
    "            'เนื้อร้อง/ทำนอง', 'ทำนอง/เรียบเรียง ', 'เพลงประกอบภาพยนตร์ ', \n",
    "            'เพลงประกอบละครซิทคอม ', 'คำร้อง/ทำนอง/เรียบเรียง ', \n",
    "            'คำร้อง/เรียบเรียง ', 'เพลงประกอบ ', 'ร้องโดย ', \n",
    "            'ทำนอง / เรียบเรียง :', ' สังกัด'\n",
    "        ]\n",
    "        if any(line.startswith(s) for s in headers):\n",
    "            pass\n",
    "        else:\n",
    "            line = ' '.join(line.replace('(', ' ').replace(')', ' ').replace('-', ' ').split())\n",
    "            lyrics_clean.append(line)\n",
    "    return '\\n'.join(lyrics_clean).strip()\n",
    "\n",
    "\n",
    "def create_lookup_dict(tokenized_lyrics, n_min=None):\n",
    "    \"\"\"\n",
    "    Create lookup dictionary\n",
    "    \"\"\"\n",
    "    word_counts = Counter(tokenized_lyrics)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    if n_min is not None:\n",
    "        sorted_vocab = {k: v for k, v in word_counts.items() if v >= n_min}\n",
    "    vocab_to_int = {word: i for i, word in enumerate(sorted_vocab, 0)}\n",
    "    int_to_vocab = {i: word for word, i in vocab_to_int.items()}\n",
    "    return (vocab_to_int, int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('lyric_dataframe.csv')\n",
    "lyrics = df.full_lyrics.map(clean_lyrics)\n",
    "tokenized_lyrics = lyrics.map(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_lyrics = flatten(tokenized_lyrics)\n",
    "tokenized_lyrics = [token if token is not '\\n' else ' ' for token in tokenized_lyrics]\n",
    "word_counts = Counter(tokenized_lyrics)\n",
    "vocab_to_int, int_to_vocab = create_lookup_dict(tokenized_lyrics, n_min=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size, shuffle=False):\n",
    "    \"\"\"\n",
    "    ref: Udacity\n",
    "    \"\"\"\n",
    "    # TODO: Implement function\n",
    "    batch_size_total = batch_size * sequence_length\n",
    "    n_batches = len(words) // batch_size_total\n",
    "    words = words[: n_batches * batch_size_total]\n",
    "    \n",
    "    X, target = [], []\n",
    "    for n in range(0, len(words) - sequence_length, 1):\n",
    "        x = words[n: n + sequence_length]\n",
    "        y = words[n + sequence_length]\n",
    "        X.append(np.array(x))\n",
    "        target.append(y)\n",
    "    X = np.array(X)\n",
    "    target = np.array(target)\n",
    "    dataset = TensorDataset(torch.from_numpy(X), torch.from_numpy(target))\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_indices = [vocab_to_int.get(token, 0) for token in tokenized_lyrics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LSTM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        super(RNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size, \n",
    "                                      embedding_dim=self.embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim, \n",
    "                            hidden_size=self.hidden_dim, \n",
    "                            dropout=self.dropout,\n",
    "                            num_layers=self.n_layers,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.output_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        batch_size, _ = nn_input.size() # batch first\n",
    "        embedding_input = self.embedding(nn_input)\n",
    "        nn_output, hidden = self.lstm(embedding_input, hidden)\n",
    "        nn_output = nn_output.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        output = self.fc(nn_output)\n",
    "        output = output.view(batch_size, -1, self.output_size)\n",
    "        output = output[:, -1]\n",
    "\n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return output, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        # initialize hidden state with zero weights, and move to GPU if available\n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    \n",
    "    # move data to GPU, if available\n",
    "    if train_on_gpu:\n",
    "        inp, target = inp.cuda(), target.cuda()\n",
    "    \n",
    "    hidden = tuple([each.data for each in hidden])\n",
    "    \n",
    "    # perform backpropagation and optimization\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    output, hidden = rnn(inp, hidden)\n",
    "    loss = criterion(output.squeeze(), target)\n",
    "    loss.backward()\n",
    "    \n",
    "    clip = 5.0 # gradient clipping\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), clip)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    \n",
    "    batch_losses = []\n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            # forward, back prop\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            # printing loss stats\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "\n",
    "    # returns a trained rnn\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "num_epochs = 15\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "vocab_size = len(vocab_to_int)\n",
    "output_size = len(vocab_to_int)\n",
    "embedding_dim = 300\n",
    "hidden_dim = 300\n",
    "sequence_length = 20\n",
    "n_layers = 2\n",
    "show_every_n_batches = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = batch_data(tokenized_indices, \n",
    "                          sequence_length=20, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "rnn = RNN(vocab_size, output_size, embedding_dim, \n",
    "          hidden_dim, n_layers, dropout=0.3)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training the model\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001)\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, \n",
    "                        criterion, num_epochs, \n",
    "                        show_every_n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.0002)\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, \n",
    "                        criterion, 15, \n",
    "                        show_every_n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(trained_rnn, 'lstm_model.pt.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lyrics generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# load trained model\n",
    "trained_rnn = torch.load('lstm_model.pt')\n",
    "\n",
    "def generate(rnn, prime_id, int_to_vocab, pad_value, predict_len=100):\n",
    "    rnn.eval()\n",
    "    \n",
    "    # create a sequence (batch_size=1) with the prime_id\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)\n",
    "        \n",
    "        # initialize the hidden state\n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "        \n",
    "        # get the output of the rnn\n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "        \n",
    "        # get the next word probabilities\n",
    "        p = F.softmax(output, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "         \n",
    "        # use top_k sampling to get the index of the next word\n",
    "        top_k = 100\n",
    "        p, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next word index with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    "        \n",
    "        # retrieve that word from the dictionary\n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)\n",
    "        \n",
    "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
    "        current_seq = np.roll(current_seq, -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "    gen_sentences = ''.join(predicted)    \n",
    "    return gen_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_length = 200\n",
    "prime_word = 'ใคร'\n",
    "generated_script = generate(trained_rnn, vocab_to_int.get(prime_word, 0), \n",
    "                            int_to_vocab, 0, gen_length)\n",
    "print(generated_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TSNE of embedding layer**\n",
    "\n",
    "Here, we will sample embedddings from the embedding layer and use TSNE to visualize the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = torch.load('lstm_model.pt')\n",
    "word_vectors = torch.Tensor(list(rnn.embedding.parameters())[0].cpu().detach().numpy())\n",
    "tsne = TSNE(n_components=2)\n",
    "\n",
    "word_vectors_proj = tsne.fit_transform(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_proj_df = pd.DataFrame(list(zip(list(vocab_to_int.keys()), \n",
    "                                             word_vectors_proj[:, 0], \n",
    "                                             word_vectors_proj[:, 1])), \n",
    "                                    columns=['word', 'x', 'y'])\n",
    "word_counts_df = pd.DataFrame(word_counts.most_common(n=1500), \n",
    "                              columns=['word', 'n_word'])\n",
    "word_vectors_proj_sel = word_vectors_proj_df.merge(word_counts_df, on='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rc('figure', figsize=(20, 20))\n",
    "\n",
    "df = word_vectors_proj_sel.sample(n=300) # sample only 300 words to show\n",
    "p = sns.regplot(data=df, \n",
    "                x=\"x\", y=\"y\", fit_reg=False, \n",
    "                marker=\"+\", color=\"skyblue\")\n",
    "for _, r in df.iterrows():\n",
    "     p.text(r['x'], r['y'], r['word'], \n",
    "            horizontalalignment='left', \n",
    "            size='large', color='black', weight='semibold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Snippet to scrape data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_siamzone(d):\n",
    "    soup = BeautifulSoup(requests.get('https://www.siamzone.com/music/thailyric/%d' % d).content, 'html.parser')\n",
    "    title, artist_name = soup.find('title').text.split('|')\n",
    "    title, artist_name = title.strip(), artist_name.strip()\n",
    "    n_shares = int(soup.find('span', attrs={'class': 'sz-social-number'}).text.replace(',', ''))\n",
    "    full_lyrics = soup.find('div', attrs={'itemprop': 'articleBody'}).text.strip()\n",
    "    return {\n",
    "        'url': 'https://www.siamzone.com/music/thailyric/%d' % d,\n",
    "        'soup': soup, \n",
    "        'title': title,\n",
    "        'artist_name': artist_name,\n",
    "        'n_shares': n_shares,\n",
    "        'full_lyrics': full_lyrics\n",
    "    }\n",
    "\n",
    "scraped_siamzone = []\n",
    "for i in range(14050, 16041):\n",
    "    try:\n",
    "        scraped_siamzone.append(scrape_siamzone(i))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "scraped_siamzone_df = pd.DataFrame(scraped_siamzone)\n",
    "scraped_siamzone_df['lyrics'] = scraped_siamzone_df.full_lyrics.map(clean_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
