{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prepare data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import chain\n",
    "\n",
    "def flatten(ls):\n",
    "    \"\"\"\n",
    "    Flatten list of list\n",
    "    \"\"\"\n",
    "    return list(chain.from_iterable(ls))\n",
    "\n",
    "def clean_tokens(tokens):\n",
    "    #tokens = [token.strip() for token in tokens if token.strip() not in [\"\", \"(\", \")\", \"*\", \".\"]]\n",
    "    \n",
    "    tokens = [token for token in tokens if token.strip() not in [\"(\", \")\", \"*\", \".\"]]\n",
    "    tokens = [token.strip() if token.strip() != \"\" else \" \" for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "siamzone_df = pd.read_pickle(\"siamzone-process-v2.pickle\")\n",
    "siamzone_trian_df, siamzone_val_df = train_test_split(siamzone_df, test_size=0.15, random_state=126)\n",
    "\n",
    "train_lyrics = flatten(siamzone_trian_df.tokenized_lyrics.map(clean_tokens))\n",
    "val_lyrics = flatten(siamzone_val_df.tokenized_lyrics.map(clean_tokens))\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Create Model**\n",
    "\n",
    "ref: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "and: https://colab.research.google.com/drive/1u34ME_e1wezCmCRdjTiTHlWg9DZsfheb?usp=sharing&fbclid=IwAR3koG2yz6nAnXYCeg7aC-rSBgmWwrX8mRgWggZCK630hDvbKZZvH6Q1QRk#scrollTo=eGMG9zFDscev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, Transformer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data import Field, BPTTIterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.datasets import LanguageModelingDataset\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "def create_lookup_dict(tokenized_lyrics, n_min=None):\n",
    "    \"\"\"\n",
    "    Create lookup dictionary from list of words (lyrics)\n",
    "    \"\"\"\n",
    "    word_counts = Counter(tokenized_lyrics)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    if n_min is not None:\n",
    "        sorted_vocab = {k: v for k, v in word_counts.items() if v >= n_min}\n",
    "    vocab_to_int = {word: i for i, word in enumerate(sorted_vocab, 1)}\n",
    "    vocab_to_int[\"<unk>\"] = 0\n",
    "    int_to_vocab = {i: word for word, i in vocab_to_int.items()}\n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "    \n",
    "def batch_data(words, sequence_length, batch_size, shuffle=False):\n",
    "    \"\"\"\n",
    "    ref: Udacity Deep learning class\n",
    "    \"\"\"\n",
    "    batch_size_total = batch_size * sequence_length\n",
    "    n_batches = len(words) // batch_size_total\n",
    "    words = words[: n_batches * batch_size_total]\n",
    "    \n",
    "    X, target = [], []\n",
    "    for n in range(0, len(words) - sequence_length, 1):\n",
    "        x = words[n: n + sequence_length]\n",
    "        y = words[n + sequence_length]\n",
    "        X.append(np.array(x))\n",
    "        target.append(y)\n",
    "    X = np.array(X)\n",
    "    target = np.array(target)\n",
    "    dataset = TensorDataset(torch.from_numpy(X), torch.from_numpy(target))\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab = create_lookup_dict(train_lyrics + val_lyrics, n_min=3)\n",
    "tokenized_indices_train = [vocab_to_int.get(token, 0) for token in train_lyrics]\n",
    "tokenized_indices_val = [vocab_to_int.get(token, 0) for token in val_lyrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int[\" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump({\"vocab_to_int\": vocab_to_int, \"int_to_vocab\": int_to_vocab},  open( \"vocab_transformer_space.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab_to_int) # the size of vocabulary\n",
    "emsize = 512 # embedding dimension\n",
    "nhid = 512 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 4 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 4 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    data = torch.LongTensor(data).unsqueeze(0).T\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 64\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(tokenized_indices_train + tokenized_indices_val, batch_size)\n",
    "val_data = batchify(tokenized_indices_val, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is arranged in column\n",
    "print([int_to_vocab.get(idx, '') for idx in train_data[:, 0].tolist()[0: 100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model, data_source):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    for batch, i in enumerate(range(0, data_source.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(data_source, i)\n",
    "        optimizer.zero_grad()\n",
    "        if data.size(0) != bptt:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(data_source) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "    \n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    ntokens = len(vocab_to_int)\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            if data.size(0) != bptt:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            output = eval_model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train from scratch do not run this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "optimizer_save_path = \"optimizer-lm-siamzone-v4-space-342.pkl\"\n",
    "optimizer.load_state_dict(torch.load(optimizer_save_path))\n",
    "\n",
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = 3\n",
    "\n",
    "    \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"lm-siamzone-v4-space-342.pkl\"\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 2 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, train_data)\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"lm-siamzone-v4-space-342.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_save_path = \"optimizer-{}\".format(model_save_path)\n",
    "torch.save(optimizer.state_dict(), optimizer_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generate new lyrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepcut\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def top_k_top_p_filtering(\n",
    "    logits, top_k, top_p, temperature, filter_value=-float(\"Inf\")\n",
    "):\n",
    "    # Hugging Face script to apply top k and nucleus sampling\n",
    "    logits = logits / temperature\n",
    "\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "def predict(model, start_word='อยากจะไป', size=50):\n",
    "    start_ids = torch.LongTensor([vocab_to_int.get(w, 0) for w in deepcut.tokenize(start_word)])\n",
    "    #print(start_ids)\n",
    "\n",
    "    outputs = []\n",
    "    outputs.extend(start_ids.tolist())\n",
    "    for i in range(size):\n",
    "        data = torch.tensor(outputs).unsqueeze(-1).to(device)\n",
    "        src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        logits = model.forward(data, src_mask)\n",
    "        top_k, top_p, temperature = 100, 0.95, 1\n",
    "        filtered_logits = top_k_top_p_filtering(logits[-1].squeeze(), top_k=top_k, top_p=top_p, temperature=temperature)\n",
    "        probabilities = F.softmax(filtered_logits, dim=-1)\n",
    "        probabilities_logits, probabilities_position = torch.sort(\n",
    "            probabilities, descending=True\n",
    "        )\n",
    "        predicted_token = torch.multinomial(probabilities, 1)\n",
    "        outputs.append(int(predicted_token))\n",
    "\n",
    "    return [int_to_vocab.get(idx, ' ') for idx in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_word = \"คิดถึงเธอ\"\n",
    "for i in range(10):\n",
    "    pred = predict(model, start_word)\n",
    "    print(''.join(pred))\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load weight and fine-tune**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_names = ['คาราบาว', 'ปู พงษ์สิทธิ์ คำภีร์']\n",
    "batch_size = 20\n",
    "\n",
    "finetune_lyrics = flatten(siamzone_df[siamzone_df.artist_name.isin(artist_names)].tokenized_lyrics_word_only)\n",
    "tokenized_indices_ft = [vocab_to_int.get(token, 0) for token in finetune_lyrics]\n",
    "finetune_data = batchify(tokenized_indices_ft, batch_size)\n",
    "finetune_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fine tuning\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.decoder = nn.Linear(emsize, ntokens).cuda()\n",
    "\n",
    "lr = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 20, gamma=0.9)\n",
    "finetune_epochs = 200\n",
    "\n",
    "for epoch in range(1, finetune_epochs + 1):\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    train(model, finetune_data)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_word = \"กัญชา\"\n",
    "for i in range(10):\n",
    "    pred = predict(model, start_word)\n",
    "    print(''.join(pred))\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
